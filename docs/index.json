[
{
	"uri": "http://rpi-data.analyticsdojo.com/modules/linearregression/",
	"title": "Linear Regression",
	"tags": [],
	"description": "",
	"content": "\n   This project will work on how to predict the prices of homes based on the properties of the house. I will determine which house affected the final sale price and how effectively we can predict the sale price.Here's a brief description of the columns in the data:\n   Learning Objectives\u0026#182;By the end of this notebook, the reader should be able to perform Linear Regression techniques in python. This includes:\n Importing and formating data Training the LinearRegression model from the sklearn.linear_model library Work with qualitative and quantitative data, and effectively deal with instances of categorical data. Analyze and determine proper handling of redundant and/or inconsistent data features. Create a heatmap visual with matplot.lib library     Read Data\u0026#182;The pandas library is an open source data analytics tool for python that allows the use of 'data frame' objects and clean file parsing.\nHere we split the Ames housing data into training and testing data. The dataset contains 82 columns which are known as features of the data. Here are a few:\n Lot Area: Lot size in square feet. Overall Qual: Rates the overall material and finish of the house. Overall Cond: Rates the overall condition of the house. Year Built: Original construction date. Low Qual Fin SF: Low quality finished square feet (all floors). Full Bath: Full bathrooms above grade. Fireplaces: Number of fireplaces.  and so on.\n   In\u0026nbsp;[1]: import pandas as pd data = pd.read_csv(\u0026quot;Data/AmesHousing.txt\u0026quot;, delimiter = \u0026#39;\\t\u0026#39;) train = data[0:1460] test = data[1460:] target = \u0026#39;SalePrice\u0026#39; print(train.info())    \n \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 1460 entries, 0 to 1459 Data columns (total 82 columns): Order 1460 non-null int64 PID 1460 non-null int64 MS SubClass 1460 non-null int64 MS Zoning 1460 non-null object Lot Frontage 1211 non-null float64 Lot Area 1460 non-null int64 Street 1460 non-null object Alley 109 non-null object Lot Shape 1460 non-null object Land Contour 1460 non-null object Utilities 1460 non-null object Lot Config 1460 non-null object Land Slope 1460 non-null object Neighborhood 1460 non-null object Condition 1 1460 non-null object Condition 2 1460 non-null object Bldg Type 1460 non-null object House Style 1460 non-null object Overall Qual 1460 non-null int64 Overall Cond 1460 non-null int64 Year Built 1460 non-null int64 Year Remod/Add 1460 non-null int64 Roof Style 1460 non-null object Roof Matl 1460 non-null object Exterior 1st 1460 non-null object Exterior 2nd 1460 non-null object Mas Vnr Type 1449 non-null object Mas Vnr Area 1449 non-null float64 Exter Qual 1460 non-null object Exter Cond 1460 non-null object Foundation 1460 non-null object Bsmt Qual 1420 non-null object Bsmt Cond 1420 non-null object Bsmt Exposure 1419 non-null object BsmtFin Type 1 1420 non-null object BsmtFin SF 1 1459 non-null float64 BsmtFin Type 2 1419 non-null object BsmtFin SF 2 1459 non-null float64 Bsmt Unf SF 1459 non-null float64 Total Bsmt SF 1459 non-null float64 Heating 1460 non-null object Heating QC 1460 non-null object Central Air 1460 non-null object Electrical 1460 non-null object 1st Flr SF 1460 non-null int64 2nd Flr SF 1460 non-null int64 Low Qual Fin SF 1460 non-null int64 Gr Liv Area 1460 non-null int64 Bsmt Full Bath 1459 non-null float64 Bsmt Half Bath 1459 non-null float64 Full Bath 1460 non-null int64 Half Bath 1460 non-null int64 Bedroom AbvGr 1460 non-null int64 Kitchen AbvGr 1460 non-null int64 Kitchen Qual 1460 non-null object TotRms AbvGrd 1460 non-null int64 Functional 1460 non-null object Fireplaces 1460 non-null int64 Fireplace Qu 743 non-null object Garage Type 1386 non-null object Garage Yr Blt 1385 non-null float64 Garage Finish 1385 non-null object Garage Cars 1460 non-null float64 Garage Area 1460 non-null float64 Garage Qual 1385 non-null object Garage Cond 1385 non-null object Paved Drive 1460 non-null object Wood Deck SF 1460 non-null int64 Open Porch SF 1460 non-null int64 Enclosed Porch 1460 non-null int64 3Ssn Porch 1460 non-null int64 Screen Porch 1460 non-null int64 Pool Area 1460 non-null int64 Pool QC 1 non-null object Fence 297 non-null object Misc Feature 60 non-null object Misc Val 1460 non-null int64 Mo Sold 1460 non-null int64 Yr Sold 1460 non-null int64 Sale Type 1460 non-null object Sale Condition 1460 non-null object SalePrice 1460 non-null int64 dtypes: float64(11), int64(28), object(43) memory usage: 935.4+ KB None     \n The train data over here will be used to create the linear regression model, while the test data will be used to figure out the accuracy of the linear regression model.\n   Use linear regression to model the data\u0026#182; In this case, we will use the simple linear regression to evaluate the relationship between 2 variable\u0026ndash;living area(\u0026ldquo;Gr Liv Area\u0026rdquo;) and price(\u0026ldquo;SalePrice\u0026rdquo;). linearRegression.fit() is a pretty convinent function that it can turn the input data into the linear function and you dont need to worry about the calculation. We can also use the mean_squared_error to get the total cariance of the linear function.\n   In\u0026nbsp;[2]: import numpy as np from sklearn.linear_model import LinearRegression\nlr = LinearRegression() lr.fit(train[[\u0026#39;Gr Liv Area\u0026#39;]], train[\u0026#39;SalePrice\u0026#39;]) from sklearn.metrics import mean_squared_error train_predictions = lr.predict(train[[\u0026#39;Gr Liv Area\u0026#39;]]) test_predictions = lr.predict(test[[\u0026#39;Gr Liv Area\u0026#39;]])\ntrain_mse = mean_squared_error(train_predictions, train[\u0026#39;SalePrice\u0026#39;]) test_mse = mean_squared_error(test_predictions, test[\u0026#39;SalePrice\u0026#39;])\ntrain_rmse = np.sqrt(train_mse) test_rmse = np.sqrt(test_mse)\nprint(lr.coef_) print(train_rmse) print(test_rmse) \n  \n [116.86624683] 56034.362001412796 57088.25161263909     \n In this case, we use the lr.coef_() to get the coefficient of the linear function which is 116.87. More than that, the standard error for the train data is 56034 and test data is 57088. Now, let\u0026rsquo;s make the result more visible by plotting.\nFollowing is the linear regression line made from data in \u0026ldquo;train\u0026rdquo;.\n   In\u0026nbsp;[4]: import matplotlib.pyplot as plt plt.scatter(train[[\u0026#39;Gr Liv Area\u0026#39;]], train[[\u0026#39;SalePrice\u0026#39;]], color=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Gr Liv Area in Train\u0026#39;, fontsize = \u0026#39;18\u0026#39;) plt.ylabel(\u0026#39;train_predictions\u0026#39; ,fontsize = \u0026#39;18\u0026#39;) trainPlot =plt.plot(train[[\u0026#39;Gr Liv Area\u0026#39;]], train_predictions, color=\u0026#39;blue\u0026#39;, linewidth=3) trainPlot \n  \nOut[4]: [\u0026lt;matplotlib.lines.Line2D at 0x1a19271d68\u0026gt;]  \n  \n \n And now lets put the model into test data set to see if it can predict the value precisely.\n   In\u0026nbsp;[5]: import matplotlib.pyplot as plt plt.scatter(test[[\u0026#39;Gr Liv Area\u0026#39;]], test[[\u0026#39;SalePrice\u0026#39;]], color=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Gr Liv Area in Test\u0026#39;, fontsize = \u0026#39;18\u0026#39;) plt.ylabel(\u0026#39;test_predictions\u0026#39; ,fontsize = \u0026#39;18\u0026#39;) testPlot = plt.plot(test[[\u0026#39;Gr Liv Area\u0026#39;]], test_predictions, color=\u0026#39;blue\u0026#39;, linewidth=3) testPlot \n  \nOut[5]: [\u0026lt;matplotlib.lines.Line2D at 0x1a192b3ac8\u0026gt;]  \n  \n \n Since all the data looks like concentrate on the linear regression model, we should conclude that the model can predict the \u0026ldquo;Sale Price\u0026rdquo; .\n   Use Multiple Regression to model the data\u0026#182; In the real world, Multiple Regression is a more useful technique since we need to evaluate more than one correlation in most cases. Now, we will still predict the SalePrice, but with one more variable \u0026ndash; Overall Condition (Overall Cond). In this case the model will be a Binary Linear Equation in the form of : $$ Y = a0 + coef{Cond} * (Overall Cond) + coef_{Area} * (Gr Liv Area) $$\n$a0$ stands for the intial value while both \u0026ldquo;Overall Cond\u0026rdquo; and \u0026ldquo;Gr Liv Area\u0026rdquo; is zero\n$coef{Cond}$ stands for the coefficient of Overall Cond\n$coef_{Area}$ stands for the coefficient of Gr Liv Area\n   In\u0026nbsp;[6]: from sklearn.metrics import mean_squared_error cols = [\u0026#39;Overall Cond\u0026#39;, \u0026#39;Gr Liv Area\u0026#39;] lr.fit(train[cols], train[\u0026#39;SalePrice\u0026#39;]) train_predictions = lr.predict(train[cols]) test_predictions = lr.predict(test[cols])\ntrain_rmse_2 = np.sqrt(mean_squared_error(train_predictions, train[\u0026#39;SalePrice\u0026#39;])) test_rmse_2 = np.sqrt(mean_squared_error(test_predictions, test[\u0026#39;SalePrice\u0026#39;]))\nprint(lr.coef) print(lr.intercept) print(train_rmse_2) print(test_rmse_2) \n  \n [-409.56846611 116.73118339] 7858.691146390513 56032.398015258674 57066.90779448559     \n such that the linear model will be like: $$ Y = 7858.7 - 409.6 * (Overall Cond) + 116.7 * (Gr Liv Area) $$\nHowever, it\u0026rsquo;s hard to make a geometric explanation since the model will be either surface or high-dimension which cant be plotted.\n   Handling data types with missing values/non-numeric values\u0026#182;In the machine learning workflow, once we\u0026rsquo;ve selected the model we want to use, selecting the appropriate features for that model is the next important step. In the following code snippets, I will explore how to use correlation between features and the target column, correlation between features, and variance of features to select features.\nI will specifically focus on selecting from feature columns that don\u0026rsquo;t have any missing values or don\u0026rsquo;t need to be transformed to be useful (e.g. columns like Year Built and Year Remod/Add).\n   In\u0026nbsp;[7]: numerical_train = train.select_dtypes(include=[\u0026#39;int64\u0026#39;, \u0026#39;float\u0026#39;]) numerical_train = numerical_train.drop([\u0026#39;PID\u0026#39;, \u0026#39;Year Built\u0026#39;, \u0026#39;Year Remod/Add\u0026#39;, \u0026#39;Garage Yr Blt\u0026#39;, \u0026#39;Mo Sold\u0026#39;, \u0026#39;Yr Sold\u0026#39;], axis=1) null_series = numerical_train.isnull().sum() full_cols_series = null_series[null_series == 0] print(full_cols_series) \n  \n Order 0 MS SubClass 0 Lot Area 0 Overall Qual 0 Overall Cond 0 1st Flr SF 0 2nd Flr SF 0 Low Qual Fin SF 0 Gr Liv Area 0 Full Bath 0 Half Bath 0 Bedroom AbvGr 0 Kitchen AbvGr 0 TotRms AbvGrd 0 Fireplaces 0 Garage Cars 0 Garage Area 0 Wood Deck SF 0 Open Porch SF 0 Enclosed Porch 0 3Ssn Porch 0 Screen Porch 0 Pool Area 0 Misc Val 0 SalePrice 0 dtype: int64     \n Correlating feature columns with Target Columns\u0026#182; I will show the the correlation between feature columns and target columns(SalesPrice) by percentage.\n   In\u0026nbsp;[8]: train_subset = train[full_cols_series.index] corrmat = train_subset.corr() sorted_corrs = corrmat[\u0026#39;SalePrice\u0026#39;].abs().sort_values() print(sorted_corrs) \n  \n Misc Val 0.009903 3Ssn Porch 0.038699 Low Qual Fin SF 0.060352 Order 0.068181 MS SubClass 0.088504 Overall Cond 0.099395 Screen Porch 0.100121 Bedroom AbvGr 0.106941 Kitchen AbvGr 0.130843 Pool Area 0.145474 Enclosed Porch 0.165873 2nd Flr SF 0.202352 Half Bath 0.272870 Lot Area 0.274730 Wood Deck SF 0.319104 Open Porch SF 0.344383 TotRms AbvGrd 0.483701 Fireplaces 0.485683 Full Bath 0.518194 1st Flr SF 0.657119 Garage Area 0.662397 Garage Cars 0.663485 Gr Liv Area 0.698990 Overall Qual 0.804562 SalePrice 1.000000 Name: SalePrice, dtype: float64     \n Correlation Matrix Heatmap\u0026#182;We now have a decent list of candidate features to use in our model, sorted by how strongly they\u0026rsquo;re correlated with the SalePrice column. For now, I will keep only the features that have a correlation of 0.3 or higher. This cutoff is a bit arbitrary and, in general, it\u0026rsquo;s a good idea to experiment with this cutoff. For example, you can train and test models using the columns selected using different cutoffs and see where your model stops improving.\nThe next thing we need to look for is for potential collinearity between some of these feature columns. Collinearity is when 2 feature columns are highly correlated and stand the risk of duplicating information. If we have 2 features that convey the same information using 2 different measures or metrics, we need to choose just one or predictive accuracy can suffer.\nWhile we can check for collinearity between 2 columns using the correlation matrix, we run the risk of information overload. We can instead generate a correlation matrix heatmap using Seaborn to visually compare the correlations and look for problematic pairwise feature correlations. Because we\u0026rsquo;re looking for outlier values in the heatmap, this visual representation is easier.\n   In\u0026nbsp;[9]: import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10,6)) strong_corrs = sorted_corrs[sorted_corrs \u0026gt; 0.3] corrmat = train_subset[strong_corrs.index].corr() ax = sns.heatmap(corrmat) ax \n  \nOut[9]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19605668\u0026gt;  \n  \n \n Train and Test the model\u0026#182;Based on the correlation matrix heatmap, we can tell that the following pairs of columns are strongly correlated:\n Gr Liv Area and TotRms AbvGrd Garage Area and Garage Cars  We will only use one of these pairs and remove any columns with missing values\n   In\u0026nbsp;[11]: final_corr_cols = strong_corrs.drop([\u0026#39;Garage Cars\u0026#39;, \u0026#39;TotRms AbvGrd\u0026#39;]) features = final_corr_cols.drop([\u0026#39;SalePrice\u0026#39;]).index target = \u0026#39;SalePrice\u0026#39; clean_test = test[final_corr_cols.index].dropna()\nlr = LinearRegression() lr.fit(train[features], train[\u0026#39;SalePrice\u0026#39;])\ntrain_predictions = lr.predict(train[features]) test_predictions = lr.predict(clean_test[features])\ntrain_mse = mean_squared_error(train_predictions, train[target]) test_mse = mean_squared_error(test_predictions, clean_test[target])\ntrain_rmse = np.sqrt(train_mse) test_rmse = np.sqrt(test_mse)\nprint(train_rmse) print(test_rmse) \n  \n 34173.97629185851 41032.026120197705     \n Removing low variance features\u0026#182;The last technique I will explore is removing features with low variance. When the values in a feature column have low variance, they don\u0026rsquo;t meaningfully contribute to the model\u0026rsquo;s predictive capability. On the extreme end, let\u0026rsquo;s imagine a column with a variance of 0. This would mean that all of the values in that column were exactly the same. This means that the column isn\u0026rsquo;t informative and isn\u0026rsquo;t going to help the model make better predictions.\nTo make apples to apples comparisions between columns, we need to standardize all of the columns to vary between 0 and 1. Then, we can set a cutoff value for variance and remove features that have less than that variance amount.\n   In\u0026nbsp;[12]: unit_train = train[features]/(train[features].max()) sorted_vars = unit_train.var().sort_values() print(sorted_vars) \n  \n Open Porch SF 0.013938 Gr Liv Area 0.018014 Full Bath 0.018621 1st Flr SF 0.019182 Overall Qual 0.019842 Garage Area 0.020347 Wood Deck SF 0.033064 Fireplaces 0.046589 dtype: float64     \n Final Model\u0026#182;Let\u0026rsquo;s set a cutoff variance of 0.015, remove the Open Porch SF feature, and train and test a model using the remaining features.\n   In\u0026nbsp;[13]: features = features.drop([\u0026#39;Open Porch SF\u0026#39;])\nclean_test = test[final_corr_cols.index].dropna()\nlr = LinearRegression() lr.fit(train[features], train[\u0026#39;SalePrice\u0026#39;])\ntrain_predictions = lr.predict(train[features]) test_predictions = lr.predict(clean_test[features])\ntrain_mse = mean_squared_error(train_predictions, train[target]) test_mse = mean_squared_error(test_predictions, clean_test[target])\ntrain_rmse_2 = np.sqrt(train_mse) test_rmse_2 = np.sqrt(testmse) print(lr.intercept) print(lr.coef_) print(train_rmse_2) print(test_rmse_2) \n  \n -112764.87061464708 [ 37.88152677 7086.98429942 -2221.97281278 43.18536387 64.88085639 38.71125489 24553.18365123] 34372.696707783965 40591.42702437726     \n The final model will be a 7-dimension linear function which looks like: $$ Y = -112765 + 37.9 * Wood Deck + 7087 * Fire Places - 2222 * Full Bath + 43 * 1st Fle SF + 65 * garage Area + 39 * Liv area + 24553 * Overall Qual $$\n   Feature transformation\u0026#182;To understand how linear regression works, I have stuck to using features from the training dataset that contained no missing values and were already in a convenient numeric representation. In this mission, we\u0026rsquo;ll explore how to transform some of the the remaining features so we can use them in our model. Broadly, the process of processing and creating new features is known as feature engineering.\n   In\u0026nbsp;[14]: train = data[0:1460] test = data[1460:] train_null_counts = train.isnull().sum() df_no_mv = train[train_null_counts[train_null_counts==0].index] \n  \n Categorical Features\u0026#182;You\u0026rsquo;ll notice that some of the columns in the data frame df_no_mv contain string values. To use these features in our model, we need to transform them into numerical representations\n   In\u0026nbsp;[15]: text_cols = df_no_mv.select_dtypes(include=[\u0026#39;object\u0026#39;]).columns\nfor col in text_cols: print(col+\u0026quot;:\u0026quot;, len(train[col].unique())) for col in text_cols: train[col] = train[col].astype(\u0026#39;category\u0026#39;) train[\u0026#39;Utilities\u0026#39;].cat.codes.value_counts() \n  \n (\u0026#39;MS Zoning:\u0026#39;, 6) (\u0026#39;Street:\u0026#39;, 2) (\u0026#39;Lot Shape:\u0026#39;, 4) (\u0026#39;Land Contour:\u0026#39;, 4) (\u0026#39;Utilities:\u0026#39;, 3) (\u0026#39;Lot Config:\u0026#39;, 5) (\u0026#39;Land Slope:\u0026#39;, 3) (\u0026#39;Neighborhood:\u0026#39;, 26) (\u0026#39;Condition 1:\u0026#39;, 9) (\u0026#39;Condition 2:\u0026#39;, 6) (\u0026#39;Bldg Type:\u0026#39;, 5) (\u0026#39;House Style:\u0026#39;, 8) (\u0026#39;Roof Style:\u0026#39;, 6) (\u0026#39;Roof Matl:\u0026#39;, 5) (\u0026#39;Exterior 1st:\u0026#39;, 14) (\u0026#39;Exterior 2nd:\u0026#39;, 16) (\u0026#39;Exter Qual:\u0026#39;, 4) (\u0026#39;Exter Cond:\u0026#39;, 5) (\u0026#39;Foundation:\u0026#39;, 6) (\u0026#39;Heating:\u0026#39;, 6) (\u0026#39;Heating QC:\u0026#39;, 4) (\u0026#39;Central Air:\u0026#39;, 2) (\u0026#39;Electrical:\u0026#39;, 4) (\u0026#39;Kitchen Qual:\u0026#39;, 5) (\u0026#39;Functional:\u0026#39;, 7) (\u0026#39;Paved Drive:\u0026#39;, 3) (\u0026#39;Sale Type:\u0026#39;, 9) (\u0026#39;Sale Condition:\u0026#39;, 5)     C:\\Users\\dongl4\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy    Out[15]: 0 1457 2 2 1 1 dtype: int64  \n \n Dummy Coding\u0026#182;When we convert a column to the categorical data type, pandas assigns a number from 0 to n-1 (where n is the number of unique values in a column) for each value. The drawback with this approach is that one of the assumptions of linear regression is violated here. Linear regression operates under the assumption that the features are linearly correlated with the target column. For a categorical feature, however, there\u0026rsquo;s no actual numerical meaning to the categorical codes that pandas assigned for that colum. An increase in the Utilities column from 1 to 2 has no correlation value with the target column, and the categorical codes are instead used for uniqueness and exclusivity (the category associated with 0 is different than the one associated with 1).\nThe common solution is to use a technique called dummy coding\n   In\u0026nbsp;[16]: dummy_cols = pd.DataFrame() for col in text_cols: col_dummies = pd.get_dummies(train[col]) train = pd.concat([train, col_dummies], axis=1) del train[col] \n  \n In\u0026nbsp;[17]: train[\u0026#39;years_until_remod\u0026#39;] = train[\u0026#39;Year Remod/Add\u0026#39;] - train[\u0026#39;Year Built\u0026#39;] \n  \n Missing Values\u0026#182;Now I will focus on handling columns with missing values. When values are missing in a column, there are two main approaches we can take:\n Remove rows containing missing values for specific columns Pro: Rows containing missing values are removed, leaving only clean data for modeling Con: Entire observations from the training set are removed, which can reduce overall prediction accuracy Impute (or replace) missing values using a descriptive statistic from the column Pro: Missing values are replaced with potentially similar estimates, preserving the rest of the observation in the model. Con: Depending on the approach, we may be adding noisy data for the model to learn  Given that we only have 1460 training examples (with ~80 potentially useful features), we don\u0026rsquo;t want to remove any of these rows from the dataset. Let\u0026rsquo;s instead focus on imputation techniques.\nWe\u0026rsquo;ll focus on columns that contain at least 1 missing value but less than 365 missing values (or 25% of the number of rows in the training set). There\u0026rsquo;s no strict threshold, and many people instead use a 50% cutoff (if half the values in a column are missing, it\u0026rsquo;s automatically dropped). Having some domain knowledge can help with determining an acceptable cutoff value.\n   In\u0026nbsp;[18]: df_missing_values = train[train_null_counts[(train_null_counts\u0026gt;0) \u0026amp; (train_null_counts\u0026lt;584)].index]\nprint(df_missing_values.isnull().sum()) print(df_missing_values.dtypes) \n  \n Lot Frontage 249 Mas Vnr Type 11 Mas Vnr Area 11 Bsmt Qual 40 Bsmt Cond 40 Bsmt Exposure 41 BsmtFin Type 1 40 BsmtFin SF 1 1 BsmtFin Type 2 41 BsmtFin SF 2 1 Bsmt Unf SF 1 Total Bsmt SF 1 Bsmt Full Bath 1 Bsmt Half Bath 1 Garage Type 74 Garage Yr Blt 75 Garage Finish 75 Garage Qual 75 Garage Cond 75 dtype: int64 Lot Frontage float64 Mas Vnr Type object Mas Vnr Area float64 Bsmt Qual object Bsmt Cond object Bsmt Exposure object BsmtFin Type 1 object BsmtFin SF 1 float64 BsmtFin Type 2 object BsmtFin SF 2 float64 Bsmt Unf SF float64 Total Bsmt SF float64 Bsmt Full Bath float64 Bsmt Half Bath float64 Garage Type object Garage Yr Blt float64 Garage Finish object Garage Qual object Garage Cond object dtype: object     \n Inputing missing values\u0026#182;It looks like about half of the columns in df_missing_values are string columns (object data type), while about half are float64 columns. For numerical columns with missing values, a common strategy is to compute the mean, median, or mode of each column and replace all missing values in that column with that value\n   In\u0026nbsp;[19]: float_cols = df_missing_values.select_dtypes(include=[\u0026#39;float\u0026#39;]) float_cols = float_cols.fillna(df_missing_values.mean()) print(float_cols.isnull().sum()) \n  \n Lot Frontage 0 Mas Vnr Area 0 BsmtFin SF 1 0 BsmtFin SF 2 0 Bsmt Unf SF 0 Total Bsmt SF 0 Bsmt Full Bath 0 Bsmt Half Bath 0 Garage Yr Blt 0 dtype: int64     \n Conclusion\u0026#182; This note book talks about how to do linear regression in machine learning by analysing the real example \u0026ndash; Boston housing data. In this case, to do the linear regression not only means we need to figure out the correlation among all the variable, but also eliminate the variable with either insignificant influence or missing value.\n   \n  \n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/modules/",
	"title": "Notebooks",
	"tags": [],
	"description": "",
	"content": "   # Open Title Contributed By     1 MNIST Pytorch on Colab  Pytorch MNIST Jason Kuruzovich   2 Time Series on Colab  Time Series Analysis Data@RPI Team   3 PCA on Colab  Introduction to Principle Component Analysis Data@RPI Team   4 LR on Colab  Linear Regression Data@RPI Team    "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/classes/",
	"title": "Online Classes",
	"tags": [],
	"description": "",
	"content": " Data Camp As part of this class you get a free subscription to Data Camp. I can\u0026rsquo;t post it here but will provide via email. Fast-AI Deep learning class. Siraj Rival Extremely entertaining YouTube star covers a wide variety of things in machine learning.  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/modules/pca/",
	"title": "Principle Component Analysis",
	"tags": [],
	"description": "",
	"content": "\u0026lt; \n   Sections:\u0026#182; PCA (Principal Component Analysis)) Origin Learning Objective PCA  Eigenvectors Running PCA Homework \n   Origin\u0026#182; This notebook was adapted from amueller\u0026rsquo;s notebook, \u0026ldquo;1 - PCA\u0026rdquo;. Here is the link to his repository https://github.com/amueller/tutorial_ml_gkbionics.git .\nThis notebook provides examples for eigenvalues and eigenvectors in LaTeX and python.\n   Learning Objective\u0026#182;  How the Principal Componenet Analysis (PCA) works. How PCA can be used to do dimensionality reduction. Understand how PCA deals with the covariance matrix by applying eigenvectors.  \n   PCA\u0026#182; PCA can always be used to simplify the data with high dimensions (larger than 2) into 2-dimensional data by eliminating the least influntial features on the data. However, we should know the elimination of data makes the independent variable less interpretable. Before we start to deal with the PCA, we need to first learn how PCA utilizes eigenvectors to gain a diagonalization covariance matrix.\n   Eigenvectors\u0026#182; Eigenvectors and eigenvalues are the main tools used by PCA to obtain a diagnolization covariance matrix. The eigenvector is a vector whos direction will not be affected by the linear transformation, hence eigenvectors represents the direction of largest variance of data while the eigenvalue decides the magnitude of this variance in those directions.\n   Here we using a simple (2x2) matrix $A$ to explain it. $$ A = \\begin{bmatrix} 1 \u0026amp; 4 3 \u0026amp; 2 \\end{bmatrix} $$\n   In\u0026nbsp;[1]: # importing class import sympy as sp import numpy as np import numpy.linalg as lg A = np.matrix([[1,4],[3,2]]) \n  \n In general, the eigenvector $v$ of a matrix $A$ is the vector where the following holds: $$ Av = \\lambda v $$ for which $\\lambda$ stands for the eigenvalue such that linear transformation on $v$ can be defined by $\\lambda$\nAlso, we can solve the equation by: $$ Av - \\lambda v = 0 v(A-\\lambda I) = 0 $$ While $I$ is the identity matrix of A\n$$ I = A^TA = AA^T $$In this case, if $v$ is none-zero vector than $Det(A - \\lambda I) = 0$, since it cannot be invertible, and we can solve $v$ for $A$ depends on this relationship. $$ I = \\begin{bmatrix} 1 \u0026amp; 0 0 \u0026amp; 1 \\end{bmatrix} $$\n   In\u0026nbsp;[2]: def solveLambda(A = A,Lambda = sp.symbols(\u0026quot;Lambda\u0026quot;, real = True) ): I = AA.I I = np.around(I, decimals =0) return (A - LambdaI) Lambda = sp.symbols(\u0026quot;Lambda\u0026quot;, real = True) B = solveLambda(A = A, Lambda = Lambda) B \n  \nOut[2]: matrix([[-1.0*Lambda + 1, 4], [3, -1.0*Lambda + 2]], dtype=object)  \n \n $$ (A - \\lambda I) = \\begin{bmatrix} 1-\\lambda \u0026amp; 4 3 \u0026amp; 2 - \\lambda \\end{bmatrix} $$    To solve the $\\lambda$ we can use the function solve in sympy or calculating.\n   In\u0026nbsp;[3]: function = Lambda*2 - 3Lambda - 10 answer = sp.solve(function, Lambda) answer \n  \nOut[3]: [-2, 5]  \n \n In this case, $\\lambda_1 = -2$ and $\\lambda_2 = 5$, and we can figure out the eigenvectors in two cases.\nFor $\\lambda_1 = -2$\n   In\u0026nbsp;[4]: identity = np.identity(len(A)) eigenvectors_1 = A - answer[0]*identity eigenvectors_1 \n  \nOut[4]: matrix([[3.00000000000000, 4], [3, 4.00000000000000]], dtype=object)  \n \n Based on the matrix we can infer the eigenvector can be $$ v_1 = \\begin{bmatrix} -4 3\\end{bmatrix} $$\nFor $\\lambda = 5$\n   In\u0026nbsp;[5]: eigenvectors_2 = A - answer[1]*identity eigenvectors_2 \n  \nOut[5]: matrix([[-4.00000000000000, 4], [3, -3.00000000000000]], dtype=object)  \n \n Based on the matrix we can infer the eigenvector can be $$ v_2 = \\begin{bmatrix} 1\n1\\end{bmatrix} $$ All in all, the covariance matrix $A\u0026rsquo;$ now can be: $$ A\u0026rsquo; = v * A $$\nSuch that we can obtain the matrix $V$ $$ V = \\begin{bmatrix} -4 \u0026amp; 1 3 \u0026amp; 1 \\end{bmatrix} $$ where $A\u0026rsquo; = V^{-1} A V$ for the diagnalization:\n   In\u0026nbsp;[6]: V = np.matrix([[-4,1],[3,1]]) diagnalization = V.I  A  V diagnalization \n  \nOut[6]: matrix([[-2.00000000e+00, 0.00000000e+00], [-1.77635684e-15, 5.00000000e+00]])  \n \n Hence, the diagonalization covariance matrix is $$ \\begin{bmatrix} -2 \u0026amp; 0\n0 \u0026amp; 5 \\end{bmatrix} $$ Luckily, PCA can do all of this by applyng the function pca.fit_transform(x) and np.cov()\n   Generating Data\u0026#182;To talking about PCA, we first create 200 random two-dimensional data points and have a look at the raw data.\n   In\u0026nbsp;[7]: import numpy as np import matplotlib.pyplot as plt Cov = np.array([[2.9, -2.2], [-2.2, 6.5]]) X = np.random.multivariate_normal([1,2], Cov, size=200) X\n\n  \nOut[7]: array([[ 1.38383744e+00, 1.04186347e+00], [ 5.51503120e-01, 4.59397605e+00], [ 2.28938623e+00, 2.47599597e+00], [ 2.19188924e-01, 3.76187062e+00], [ 1.28817280e+00, 2.26882900e+00], [ 1.49215277e+00, 5.43773199e+00], [ 2.53729723e+00, 1.08715044e+00], [-2.10633441e+00, 5.37888151e+00], [ 1.70128797e+00, -3.75436698e+00], [-1.21499149e+00, 3.48059163e+00], [-7.54648318e-01, 5.62416517e+00], [-1.37558065e+00, 2.23346390e+00], [ 4.21464198e+00, -2.02410801e-01], [ 2.21172328e-01, 5.31123422e+00], [ 1.67220400e+00, 1.67759592e+00], [-1.08342732e+00, 7.64417741e-01], [ 1.07512204e+00, 5.06646797e+00], [ 7.51991443e-01, 5.94325847e+00], [ 3.27556307e+00, -5.53357218e-01], [ 3.86073302e+00, 2.23481202e+00], [ 1.08506670e-01, 3.07825464e+00], [ 1.51667604e+00, -5.28763254e-01], [ 2.11773305e+00, 3.46545163e+00], [ 2.38272739e+00, 3.12772008e+00], [ 2.43004643e+00, 1.30501739e+00], [ 1.47374826e+00, 2.70990735e+00], [ 2.83921401e+00, -3.98487101e+00], [-9.48501078e-01, 7.36280908e+00], [ 1.02124459e+00, 3.21448182e+00], [ 3.74240494e+00, -3.80052610e+00], [ 3.92856602e+00, -3.58849612e-02], [ 1.42662492e+00, 1.63372047e+00], [ 1.79334228e+00, -2.36036613e+00], [ 4.45535088e+00, -2.86569345e+00], [ 3.59338676e+00, -3.45793548e+00], [ 8.30053163e-01, 1.49907634e+00], [-1.21525957e+00, 2.56792684e-01], [ 8.41093256e-01, 1.22547217e+00], [-1.69501441e+00, 5.98430597e+00], [ 1.43031613e+00, 5.64150340e+00], [ 3.44927882e+00, 1.66382944e+00], [ 3.81900211e+00, -2.19543036e+00], [-3.87124208e+00, 8.29581171e+00], [ 4.00111772e+00, -4.27922754e+00], [ 3.83101981e+00, 3.29064412e+00], [ 9.81171752e-01, -4.99434251e-01], [ 1.94119265e+00, 4.67199599e+00], [ 1.70492866e+00, 1.30461463e+00], [ 4.12720768e+00, -7.26288521e-02], [-7.52312808e-01, -1.10538573e+00], [ 2.23259800e+00, 5.56937461e+00], [ 5.38236269e-01, -1.90110017e-01], [ 3.16741171e+00, -1.44677173e+00], [-1.27851543e+00, 3.14513557e+00], [ 5.59405602e-01, 3.28397792e+00], [ 1.04967834e+00, 2.16292436e+00], [ 2.07759581e+00, 2.58613039e+00], [ 2.49801084e+00, -1.33600160e-01], [ 7.01636043e-02, 2.66580633e+00], [-8.22980265e-01, 1.66177175e+00], [ 1.82036215e+00, 3.64253057e+00], [-8.37296475e-01, 6.09361748e+00], [ 1.21340867e+00, 3.27451190e+00], [ 9.25208337e-01, 4.98364326e-01], [ 3.27259833e+00, 1.93455706e+00], [ 1.00070636e+00, 1.59008364e+00], [-5.68151874e-01, -1.06089282e+00], [ 9.09293762e-01, 6.51203314e+00], [ 2.47492239e-01, 7.40256013e+00], [-9.92062671e-01, 4.70095253e+00], [ 1.78360648e-01, -9.52978989e-01], [ 4.51547585e-01, 2.35927182e+00], [ 1.71056552e+00, -1.67556930e+00], [ 2.31695319e+00, -1.44061500e-01], [ 1.01846555e+00, 1.88192558e+00], [ 7.91568221e-01, -5.55651801e-01], [-1.08927425e-01, 6.83735657e+00], [ 8.59044230e-01, 3.09460584e+00], [ 2.67238279e-01, 3.06640265e+00], [ 4.19530508e-01, 1.06701189e+00], [-9.94313604e-01, 3.67096177e+00], [ 1.32071621e+00, -9.63121475e-01], [ 1.89326771e+00, 1.26373806e+00], [ 1.66854004e+00, 3.32503622e+00], [ 1.34388656e+00, 3.05995813e+00], [ 1.63738289e+00, 3.57678427e+00], [ 5.55590009e-02, 2.05752678e+00], [ 2.97838891e+00, -1.56019610e+00], [-1.76374127e+00, 3.41813325e+00], [ 3.00685070e+00, -1.77795780e+00], [ 6.31432567e-03, 2.44837593e+00], [ 2.63884511e+00, 8.79250395e-01], [-1.86971890e+00, 2.82835210e+00], [-1.65576461e+00, 1.18229879e+00], [ 1.14193225e+00, 9.81377514e-01], [ 9.67591748e-01, 3.01467398e+00], [ 8.50965697e-02, 8.88828385e-01], [ 2.44559776e-01, 1.91318437e+00], [ 2.36951662e+00, -7.60507571e-01], [ 4.05324185e+00, -5.69679189e-01], [-1.93261573e-01, -1.06964041e+00], [ 2.51804738e+00, -1.45263263e+00], [-7.94520085e-01, 2.73488878e+00], [-2.87464834e-01, 2.05286405e+00], [-8.40100523e-01, 3.48513075e+00], [-1.49514859e+00, 1.69273655e+00], [-2.58332986e+00, 4.13183244e+00], [ 2.54619863e+00, 2.38425639e+00], [ 2.98013531e+00, -2.00044713e+00], [ 7.21769592e-01, -8.21390714e-01], [ 1.27381758e+00, -1.57997158e+00], [-1.25560897e-01, 4.23214312e+00], [ 3.63441401e+00, 8.44587003e-02], [-2.36571393e-01, 3.16966703e+00], [ 4.68073279e+00, -7.10871648e-01], [ 3.01691079e+00, 1.22995633e+00], [-1.64623493e+00, 3.85439166e+00], [-1.44897118e+00, 3.70852348e+00], [ 1.94901692e+00, -3.12987251e-01], [ 1.06230454e+00, 1.33344289e+00], [ 2.85633768e+00, 2.55296997e-01], [ 1.79762588e+00, 1.01342475e+00], [-1.35918547e-01, 2.74876660e+00], [-2.19381773e+00, 5.22881102e+00], [ 8.27024246e-01, 3.33919600e+00], [ 4.40522926e+00, 3.14377449e-01], [ 1.50565782e+00, 1.23341347e+00], [ 2.19532470e+00, 3.38652837e-01], [ 2.63387430e+00, 2.33039580e+00], [ 1.57733249e+00, 1.12118265e+00], [ 5.29588084e+00, -4.76361410e+00], [-1.18380202e+00, 4.88556968e+00], [-1.38386784e-01, 4.72498304e+00], [ 3.16401438e+00, -7.17221082e-01], [-1.05810240e+00, 3.48337849e+00], [ 3.11582631e-01, 2.41335816e+00], [-1.09330171e+00, 1.73976616e+00], [ 1.84467874e+00, 6.75953722e-01], [-8.08045806e-01, 1.93311144e+00], [ 1.87554417e+00, 2.45738144e+00], [-1.93664159e+00, 5.11069364e+00], [ 2.07624724e+00, -4.69623682e-01], [-6.84527335e-01, 1.58039362e+00], [-5.16337981e-01, 3.85267200e+00], [ 5.08282993e-01, 5.18143885e+00], [-8.66529055e-01, 7.97870195e+00], [-1.56776039e+00, 5.96604318e+00], [ 1.26252149e+00, 5.07210180e-02], [ 3.56861856e-01, 3.98449204e+00], [-1.36489579e+00, 1.30205252e+00], [-2.66313742e+00, 4.20574961e+00], [-5.45195508e-01, 1.35447683e+00], [ 4.29338498e-01, 2.54727765e+00], [ 5.70758423e-01, -1.09839803e+00], [ 1.93716980e+00, 2.76739786e+00], [-2.45007695e-01, 5.13053728e+00], [ 1.34738911e+00, 9.14006988e+00], [ 1.10161001e+00, -3.76250741e+00], [-1.88153568e+00, 4.33921926e+00], [ 1.61124751e+00, 1.91403407e+00], [ 1.34838806e+00, 4.92145174e+00], [ 4.84694042e+00, -3.18014218e+00], [ 1.06905004e+00, 5.59810374e+00], [ 3.25255391e-01, 7.62121687e+00], [ 2.15554541e+00, 5.68059855e-01], [ 2.67705291e-01, 2.52002657e+00], [ 3.91708109e-01, 2.68307605e+00], [ 7.67764010e-01, 4.14442882e+00], [ 7.48390710e-01, 2.30481020e+00], [ 2.59682194e-01, 1.11248540e+00], [ 6.94497240e-01, 2.72124348e+00], [ 1.55548421e+00, -1.71005566e-01], [-1.56193106e+00, 3.62306206e+00], [-2.16684028e+00, 5.13772744e+00], [-1.82286903e+00, 5.65300036e+00], [ 1.88708350e+00, -8.80279146e-01], [ 2.35138161e+00, 1.27475964e+00], [ 6.36625481e-01, 5.98825455e+00], [ 1.33325949e+00, 3.24557416e+00], [-5.93547157e-01, -1.06895394e+00], [ 2.58666551e+00, -3.89370325e+00], [ 2.32124234e+00, 1.94765046e+00], [ 3.25631242e+00, 3.45587737e+00], [ 7.72193865e-01, 3.27678323e+00], [ 5.53229922e-01, 1.71998684e+00], [ 1.50981860e+00, -3.44209764e+00], [-1.33168718e+00, -7.50108534e-01], [-7.25131902e-01, 1.24190386e+00], [-1.62072423e+00, 7.67401802e+00], [ 1.66222391e+00, -5.34080520e-01], [ 6.43929100e-01, 1.09047234e+00], [ 1.02770779e+00, 3.72345342e+00], [-6.11966234e-01, 1.59675835e+00], [ 1.36497945e-01, 2.92899117e-01], [ 3.09811395e+00, -5.12086304e-01], [ 5.71442092e-01, 2.25865083e+00], [ 3.21450663e+00, 3.87686171e+00], [ 3.27938842e+00, -9.19335290e-01], [-1.42975483e+00, 4.54759266e+00], [ 3.02329400e+00, 1.07740589e+00]])  \n \n In\u0026nbsp;[8]: np.set_printoptions(4, suppress=True) # show only four decimals print (X[:10,:]) # print the first 10 rows of X (from 0 to 9) \n  \n [[ 1.3838 1.0419] [ 0.5515 4.594 ] [ 2.2894 2.476 ] [ 0.2192 3.7619] [ 1.2882 2.2688] [ 1.4922 5.4377] [ 2.5373 1.0872] [-2.1063 5.3789] [ 1.7013 -3.7544] [-1.215 3.4806]]     \n We round the whole data for only 4 decimals.\n   However, there is no obvious relationship based on this 2-dimensional data, hence we plot it.\n   In\u0026nbsp;[9]: plt.figure(figsize=(4,4)) plt.scatter(X[:,0], X[:,1], c= \u0026quot;b\u0026quot;, edgecolor = \u0026quot;black\u0026quot;) plt.axis(\u0026#39;equal\u0026#39;) # equal scaling on both axis; \n  \nOut[9]: (-4.344383174063181, 5.769021935148706, -5.473974894688988, 9.850430677793184)  \n  \n \n We can have a look at the actual covariance matrix,as well:\n   In\u0026nbsp;[10]: print (np.cov(X,rowvar=False)) \n  \n [[ 3.0059 -2.5038] [-2.5038 7.2065]]     \n Running PCA\u0026#182; We would now like to analyze the directions in which the data varies most. For that, we\n place the point cloud in the center (0,0) and rotate it, such that the direction with most variance is parallel to the x-axis.  Both steps can be done using PCA, which is conveniently available in sklearn.\nWe start by loading the PCA class from the sklearn package and creating an instance of the class:\n   In\u0026nbsp;[11]: from sklearn.decomposition import PCA pca = PCA() \n  \n Now, pca is an object which has a function pca.fit_transform(x) which performs both steps from above to its argument x, and returns the centered and rotated version of x.\n   In\u0026nbsp;[12]: X_pca = pca.fit_transform(X) \n  \n In\u0026nbsp;[13]: pca.components_ \n  \nOut[13]: array([[ 0.4227, -0.9063], [ 0.9063, 0.4227]])  \n \n In\u0026nbsp;[14]: pca.mean_ \n  \nOut[14]: array([0.946 , 1.9484])  \n \n In\u0026nbsp;[15]: plt.figure(figsize=(4,4)) plt.scatter(X_pca[:,0], X_pca[:,1],c = \u0026quot;b\u0026quot;, edgecolor = \u0026quot;black\u0026quot;) plt.axis(\u0026#39;equal\u0026#39;); \n  \n  \n \n The covariances between different axes should be zero now. We can double-check by having a look at the non-diagonal entries of the covariance matrix:\n   In\u0026nbsp;[16]: print (np.cov(X_pca, rowvar=False)) \n  \n [[ 8.3743 -0. ] [-0. 1.8381]]     \n High-Dimensional Data\u0026#182;Our small example above was very easy, since we could get insight into the data by simply plotting it. This approach will not work once you have more than 3 dimensions, Let\u0026rsquo;s use the famous iris dataset, which has the following 4 dimensions:\n Sepal Length Sepal Width Pedal Length Pedal Width \n   In\u0026nbsp;[17]: from io import open data = open(\u0026#39;bezdekIris.data\u0026#39;, \u0026#39;r\u0026#39;).readlines() iris_HD = np.matrix([np.array(val.split(\u0026#39;,\u0026#39;)[:4]).astype(float) for val in data[:-1]]) \n  \n Lets look at the data again. First, the raw data:\n   In\u0026nbsp;[60]: print (iris_HD[:10]) \n  \n [[-0.9007 1.019 -1.3402 -1.3154] [-1.143 -0.132 -1.3402 -1.3154] [-1.3854 0.3284 -1.3971 -1.3154] [-1.5065 0.0982 -1.2834 -1.3154] [-1.0218 1.2492 -1.3402 -1.3154] [-0.5372 1.9398 -1.1697 -1.0522] [-1.5065 0.7888 -1.3402 -1.1838] [-1.0218 0.7888 -1.2834 -1.3154] [-1.7489 -0.3622 -1.3402 -1.3154] [-1.143 0.0982 -1.2834 -1.4471]]     \n Since each dimension has different scale in the Iris Database, we can use StandardScaler to standard the unit of all dimension onto unit scale.\n   In\u0026nbsp;[61]: from sklearn.preprocessing import StandardScaler iris_HD = StandardScaler().fit_transform(iris_HD) iris_HD \n  \nOut[61]: array([[-0.9007, 1.019 , -1.3402, -1.3154], [-1.143 , -0.132 , -1.3402, -1.3154], [-1.3854, 0.3284, -1.3971, -1.3154], [-1.5065, 0.0982, -1.2834, -1.3154], [-1.0218, 1.2492, -1.3402, -1.3154], [-0.5372, 1.9398, -1.1697, -1.0522], [-1.5065, 0.7888, -1.3402, -1.1838], [-1.0218, 0.7888, -1.2834, -1.3154], [-1.7489, -0.3622, -1.3402, -1.3154], [-1.143 , 0.0982, -1.2834, -1.4471], [-0.5372, 1.4794, -1.2834, -1.3154], [-1.2642, 0.7888, -1.2266, -1.3154], [-1.2642, -0.132 , -1.3402, -1.4471], [-1.87 , -0.132 , -1.5107, -1.4471], [-0.0525, 2.17 , -1.4539, -1.3154], [-0.1737, 3.0908, -1.2834, -1.0522], [-0.5372, 1.9398, -1.3971, -1.0522], [-0.9007, 1.019 , -1.3402, -1.1838], [-0.1737, 1.7096, -1.1697, -1.1838], [-0.9007, 1.7096, -1.2834, -1.1838], [-0.5372, 0.7888, -1.1697, -1.3154], [-0.9007, 1.4794, -1.2834, -1.0522], [-1.5065, 1.2492, -1.5676, -1.3154], [-0.9007, 0.5586, -1.1697, -0.9205], [-1.2642, 0.7888, -1.056 , -1.3154], [-1.0218, -0.132 , -1.2266, -1.3154], [-1.0218, 0.7888, -1.2266, -1.0522], [-0.7795, 1.019 , -1.2834, -1.3154], [-0.7795, 0.7888, -1.3402, -1.3154], [-1.3854, 0.3284, -1.2266, -1.3154], [-1.2642, 0.0982, -1.2266, -1.3154], [-0.5372, 0.7888, -1.2834, -1.0522], [-0.7795, 2.4002, -1.2834, -1.4471], [-0.416 , 2.6304, -1.3402, -1.3154], [-1.143 , 0.0982, -1.2834, -1.3154], [-1.0218, 0.3284, -1.4539, -1.3154], [-0.416 , 1.019 , -1.3971, -1.3154], [-1.143 , 1.2492, -1.3402, -1.4471], [-1.7489, -0.132 , -1.3971, -1.3154], [-0.9007, 0.7888, -1.2834, -1.3154], [-1.0218, 1.019 , -1.3971, -1.1838], [-1.6277, -1.7434, -1.3971, -1.1838], [-1.7489, 0.3284, -1.3971, -1.3154], [-1.0218, 1.019 , -1.2266, -0.7889], [-0.9007, 1.7096, -1.056 , -1.0522], [-1.2642, -0.132 , -1.3402, -1.1838], [-0.9007, 1.7096, -1.2266, -1.3154], [-1.5065, 0.3284, -1.3402, -1.3154], [-0.6583, 1.4794, -1.2834, -1.3154], [-1.0218, 0.5586, -1.3402, -1.3154], [ 1.4015, 0.3284, 0.5354, 0.2641], [ 0.6745, 0.3284, 0.4217, 0.3958], [ 1.2803, 0.0982, 0.6491, 0.3958], [-0.416 , -1.7434, 0.1375, 0.1325], [ 0.7957, -0.5924, 0.4786, 0.3958], [-0.1737, -0.5924, 0.4217, 0.1325], [ 0.5533, 0.5586, 0.5354, 0.5274], [-1.143 , -1.5132, -0.2603, -0.2624], [ 0.9168, -0.3622, 0.4786, 0.1325], [-0.7795, -0.8226, 0.0807, 0.2641], [-1.0218, -2.4339, -0.1466, -0.2624], [ 0.0687, -0.132 , 0.2512, 0.3958], [ 0.1898, -1.9736, 0.1375, -0.2624], [ 0.311 , -0.3622, 0.5354, 0.2641], [-0.2948, -0.3622, -0.0898, 0.1325], [ 1.038 , 0.0982, 0.3649, 0.2641], [-0.2948, -0.132 , 0.4217, 0.3958], [-0.0525, -0.8226, 0.1944, -0.2624], [ 0.4322, -1.9736, 0.4217, 0.3958], [-0.2948, -1.283 , 0.0807, -0.1308], [ 0.0687, 0.3284, 0.5922, 0.7907], [ 0.311 , -0.5924, 0.1375, 0.1325], [ 0.5533, -1.283 , 0.6491, 0.3958], [ 0.311 , -0.5924, 0.5354, 0.0009], [ 0.6745, -0.3622, 0.3081, 0.1325], [ 0.9168, -0.132 , 0.3649, 0.2641], [ 1.1592, -0.5924, 0.5922, 0.2641], [ 1.038 , -0.132 , 0.7059, 0.659 ], [ 0.1898, -0.3622, 0.4217, 0.3958], [-0.1737, -1.0528, -0.1466, -0.2624], [-0.416 , -1.5132, 0.0239, -0.1308], [-0.416 , -1.5132, -0.033 , -0.2624], [-0.0525, -0.8226, 0.0807, 0.0009], [ 0.1898, -0.8226, 0.7628, 0.5274], [-0.5372, -0.132 , 0.4217, 0.3958], [ 0.1898, 0.7888, 0.4217, 0.5274], [ 1.038 , 0.0982, 0.5354, 0.3958], [ 0.5533, -1.7434, 0.3649, 0.1325], [-0.2948, -0.132 , 0.1944, 0.1325], [-0.416 , -1.283 , 0.1375, 0.1325], [-0.416 , -1.0528, 0.3649, 0.0009], [ 0.311 , -0.132 , 0.4786, 0.2641], [-0.0525, -1.0528, 0.1375, 0.0009], [-1.0218, -1.7434, -0.2603, -0.2624], [-0.2948, -0.8226, 0.2512, 0.1325], [-0.1737, -0.132 , 0.2512, 0.0009], [-0.1737, -0.3622, 0.2512, 0.1325], [ 0.4322, -0.3622, 0.3081, 0.1325], [-0.9007, -1.283 , -0.4308, -0.1308], [-0.1737, -0.5924, 0.1944, 0.1325], [ 0.5533, 0.5586, 1.2743, 1.7121], [-0.0525, -0.8226, 0.7628, 0.9223], [ 1.5227, -0.132 , 1.2175, 1.1856], [ 0.5533, -0.3622, 1.0469, 0.7907], [ 0.7957, -0.132 , 1.1606, 1.3172], [ 2.1285, -0.132 , 1.6153, 1.1856], [-1.143 , -1.283 , 0.4217, 0.659 ], [ 1.765 , -0.3622, 1.4448, 0.7907], [ 1.038 , -1.283 , 1.1606, 0.7907], [ 1.6438, 1.2492, 1.3311, 1.7121], [ 0.7957, 0.3284, 0.7628, 1.0539], [ 0.6745, -0.8226, 0.8764, 0.9223], [ 1.1592, -0.132 , 0.9901, 1.1856], [-0.1737, -1.283 , 0.7059, 1.0539], [-0.0525, -0.5924, 0.7628, 1.5805], [ 0.6745, 0.3284, 0.8764, 1.4488], [ 0.7957, -0.132 , 0.9901, 0.7907], [ 2.2497, 1.7096, 1.6722, 1.3172], [ 2.2497, -1.0528, 1.7858, 1.4488], [ 0.1898, -1.9736, 0.7059, 0.3958], [ 1.2803, 0.3284, 1.1038, 1.4488], [-0.2948, -0.5924, 0.6491, 1.0539], [ 2.2497, -0.5924, 1.6722, 1.0539], [ 0.5533, -0.8226, 0.6491, 0.7907], [ 1.038 , 0.5586, 1.1038, 1.1856], [ 1.6438, 0.3284, 1.2743, 0.7907], [ 0.4322, -0.5924, 0.5922, 0.7907], [ 0.311 , -0.132 , 0.6491, 0.7907], [ 0.6745, -0.5924, 1.0469, 1.1856], [ 1.6438, -0.132 , 1.1606, 0.5274], [ 1.8862, -0.5924, 1.3311, 0.9223], [ 2.492 , 1.7096, 1.5016, 1.0539], [ 0.6745, -0.5924, 1.0469, 1.3172], [ 0.5533, -0.5924, 0.7628, 0.3958], [ 0.311 , -1.0528, 1.0469, 0.2641], [ 2.2497, -0.132 , 1.3311, 1.4488], [ 0.5533, 0.7888, 1.0469, 1.5805], [ 0.6745, 0.0982, 0.9901, 0.7907], [ 0.1898, -0.132 , 0.5922, 0.7907], [ 1.2803, 0.0982, 0.9333, 1.1856], [ 1.038 , 0.0982, 1.0469, 1.5805], [ 1.2803, 0.0982, 0.7628, 1.4488], [-0.0525, -0.8226, 0.7628, 0.9223], [ 1.1592, 0.3284, 1.2175, 1.4488], [ 1.038 , 0.5586, 1.1038, 1.7121], [ 1.038 , -0.132 , 0.8196, 1.4488], [ 0.5533, -1.283 , 0.7059, 0.9223], [ 0.7957, -0.132 , 0.8196, 1.0539], [ 0.4322, 0.7888, 0.9333, 1.4488], [ 0.0687, -0.132 , 0.7628, 0.7907]])  \n \n We can also try plot a few two-dimensional projections, with combinations of 2 features at a time:\n   In\u0026nbsp;[52]: colorClass = [val.split(\u0026#39;,\u0026#39;)[-1].replace(\u0026#39;\\n\u0026#39;, \u0026#39;\u0026#39;) for val in data[:-1]] for i in range(len(colorClass)): val = colorClass[i] if val == \u0026#39;Iris-setosa\u0026#39;: colorClass[i] =\u0026#39;r\u0026#39; elif val == \u0026#39;Iris-versicolor\u0026#39;: colorClass[i] =\u0026#39;b\u0026#39; elif val == \u0026#39;Iris-virginica\u0026#39;: colorClass[i] =\u0026#39;g\u0026#39;\nplt.figure(figsize=(8,8)) for i in range(0,4): for j in range(0,4): plt.subplot(4, 4, i * 4 + j + 1) plt.scatter(iris_HD[:,i].tolist(), iris_HD[:,j].tolist(),c = colorClass, edgecolors = \u0026quot;black\u0026quot;) plt.axis(\u0026#39;equal\u0026#39;) plt.gca().set_aspect(\u0026#39;equal\u0026#39;) \n  \n  \n \n It is not easy to see that this is still a two-dimensional dataset!\nHowever, if we now do PCA on it, you\u0026rsquo;ll see that the last two dimensions do not matter at all:\n   In\u0026nbsp;[66]: pca = PCA() X_HE = pca.fit_transform(iris_HD) print (X_HE[:10,:]) \n  \n [[-2.2647 0.48 -0.1277 -0.0242] [-2.081 -0.6741 -0.2346 -0.103 ] [-2.3642 -0.3419 0.0442 -0.0284] [-2.2994 -0.5974 0.0913 0.066 ] [-2.3898 0.6468 0.0157 0.0359] [-2.0756 1.4892 0.027 -0.0066] [-2.444 0.0476 0.3355 0.0368] [-2.2328 0.2231 -0.0887 0.0246] [-2.3346 -1.1153 0.1451 0.0269] [-2.1843 -0.469 -0.2538 0.0399]]     \n By looking at the data after PCA, it is easy to see the value of last two dimension, especially the last one, is pretty small such that the data can be considered as still only two-dimensional. To prove this we can use the code PCA(0.95) to told PCA choose the least number of PCA components such that 95% of the data can be kept.\n   Lets give a try on it!\n   In\u0026nbsp;[74]: pca = PCA(0.95) X_95 = pca.fit_transform(iris_HD) print (X_95[:10,:]) \n  \n [[-2.2647 0.48 ] [-2.081 -0.6741] [-2.3642 -0.3419] [-2.2994 -0.5974] [-2.3898 0.6468] [-2.0756 1.4892] [-2.444 0.0476] [-2.2328 0.2231] [-2.3346 -1.1153] [-2.1843 -0.469 ]]     \n We can see that PCA eliminate  the last two dimension cause they are redundant under our requirment. Let\u0026rsquo;s plot the two dimension\n   In\u0026nbsp;[75]: plt.figure(figsize=(4,4)) plt.scatter(X_HE[:,0], X_HE[:,1], c = colorClass, edgecolor = \u0026quot;black\u0026quot;) plt.axis(\u0026#39;equal\u0026#39;) plt.gca().set_aspect(\u0026#39;equal\u0026#39;) \n  \n  \n \n We can have a look on the relationship between each dimention from following plots.\n   In\u0026nbsp;[55]: plt.figure(figsize=(8,8)) for i in range(4): for j in range(4): plt.subplot(4, 4, i * 4 + j + 1) plt.scatter(X_HE[:,i], X_HE[:,j], c = colorClass, edgecolor = \u0026quot;black\u0026quot;) plt.gca().set_xlim(-40,40) plt.gca().set_ylim(-40,40) plt.axis(\u0026#39;equal\u0026#39;) plt.gca().set_aspect(\u0026#39;equal\u0026#39;) \n  \n  \n \n It is easy to see that the correlation between other dimensions (other than first two) was ambiguous and highly concentrated in either horizontal or vertical line. This fact suggests that there are large difference between the dimension we select so that the weak dimension cant change too much on the shape of graph.\n   Dimension Reduction with PCA\u0026#182;We can see that there are actually only two dimensions in the dataset.\nLet\u0026rsquo;s throw away even more data \u0026ndash; the second dimension \u0026ndash; and reconstruct the original data in D.\n   In\u0026nbsp;[56]: pca = PCA(1) # only keep one dimension! X_E = pca.fit_transform(iris_HD) print (X_E[:10,:]) \n  \n [[-2.2647] [-2.081 ] [-2.3642] [-2.2994] [-2.3898] [-2.0756] [-2.444 ] [-2.2328] [-2.3346] [-2.1843]]     \n Now lets plot the reconstructed data and compare to the original data D. We plot the original data in red, and the reconstruction with only one dimension in blue:\n   In\u0026nbsp;[57]: X_reconstructed = pca.inverse_transform(X_E) plt.figure(figsize=(8,8)) for i in range(4): for j in range(4): plt.subplot(4, 4, i * 4 + j + 1) plt.scatter(iris_HD[:,i].tolist(), iris_HD[:,j].tolist(),c=colorClass, edgecolor = \u0026quot;black\u0026quot;) plt.scatter(X_reconstructed[:,i], X_reconstructed[:,j],c=\u0026#39;purple\u0026#39;, edgecolor = \u0026quot;black\u0026quot;) plt.axis(\u0026#39;equal\u0026#39;) \n  \n  \n \n Homework\u0026#182; 1) Do the PCA reduction on the ramdon 6-dimension data and plot it out.\n2) Explan what PCA does on your data.\n*The code for data are given.\n   In\u0026nbsp;[58]: pca=PCA(6) DATA = np.dot(X,np.random.uniform(0.2,3,(2,6))(np.random.randint(0,2,(2,6))2-1)) DATA \n  \nOut[58]: array([[ 1.831 , 0.5057, -3.3554, -3.5867, 2.2747, 3.6566], [ -1.8824, -5.7901, 1.6444, -0.1932, 6.005 , 13.0465], [ 2.5589, -0.2421, -5.0142, -5.7112, 4.6811, 8.1359], ..., [ 6.4571, 6.0567, -10.3691, -9.5022, 1.2564, -0.7318], [ -5.4073, -8.5864, 7.4795, 5.3696, 4.5117, 11.8197], [ 4.7496, 2.8237, -8.1859, -8.1907, 3.5069, 4.6639]])  \n \n In\u0026nbsp;[26]: ## Answer:\npca=PCA(6) DATA = np.dot(X,np.random.uniform(0.2,3,(2,6))(np.random.randint(0,2,(2,6))2-1)) DATA2 = pca.fit_transform(DATA)\nplt.figure(figsize=(4,4)) plt.scatter(DATA2[:,0], DATA2[:,1], c = \u0026quot;b\u0026quot;, edgecolor = \u0026quot;black\u0026quot;) plt.axis(\u0026#39;equal\u0026#39;) plt.gca().set_aspect(\u0026#39;equal\u0026#39;) \n  \n  \n \n Contributers: Linghao Dong, Josh Beck, Jose Figueroa, Yuvraj Chopra\n   \n  \n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/",
	"title": "Additional Resources",
	"tags": [],
	"description": "",
	"content": "As a data scientist, there are a wide variety of technologies you can be expected to understand and use. These are a number of recommended resources.\nPlease help to augment these resources by issuing pull requests.\nNote: As a student you are welcome to a really cool student developer pack that even faculty don\u0026rsquo;t qualify for. You can download it at https://education.github.com/pack.\n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/modules/time_series_basic/",
	"title": "Time Series Analysis",
	"tags": [],
	"description": "",
	"content": "   Time Series Analysis            A time series is a series of data points indexed (or listed or graphed) in time order. Time series analysis pertains to methods extracting meaningful statistics from time series data. This is commonly used for forecasting and other models.\n   Learning Objectives\u0026#182;  Understand the uses of Time Series Analysis Understand the pros and cons of various TSA methods, including differentiating between linear and non-linear methods. Apply the facebook prophet model and analyze the results on given rossman store data.     Sections\u0026#182;  Problem Description Exploratory Data Analysis Training Data Store Data   Moving Average Model Facebook Prophet Model Conclusion References     \nProblem Description\u0026#182; We will use the rossman store sales database for this notebook. Following is the description of Data from the website:\n\"Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied.\"\n  In\u0026nbsp;[1]: # Library Imports import numpy as np import pandas as pd import matplotlib import seaborn as sns import matplotlib.pyplot as plt import matplotlib from scipy.stats import skew from scipy.stats.stats import pearsonr from math import sqrt from sklearn.metrics import mean_squared_error import fbprophet # matplotlib parameters matplotlib.rcParams[\u0026#39;axes.labelsize\u0026#39;] = 14 matplotlib.rcParams[\u0026#39;xtick.labelsize\u0026#39;] = 12 matplotlib.rcParams[\u0026#39;ytick.labelsize\u0026#39;] = 12 matplotlib.rcParams[\u0026#39;text.color\u0026#39;] = \u0026#39;k\u0026#39; %config InlineBackend.figure_format = \u0026#39;retina\u0026#39; %matplotlib inline    \n In\u0026nbsp;[2]: # Data Reading train = pd.read_csv(\u0026#39;https://raw.githubusercontent.com/RPI-DATA/tutorials-intro/master/rossmann-store-sales/rossmann-store-sales/train.csv\u0026amp;#39;, parse_dates = True, low_memory = False, index_col = \u0026#39;Date\u0026#39;) store = pd.read_csv(\u0026#39;https://raw.githubusercontent.com/RPI-DATA/tutorials-intro/master/rossmann-store-sales/rossmann-store-sales/store.csv\u0026amp;#39;, low_memory = False) \n  \n \nExploratory Data Analysis (Train)\u0026#182; We start by seeing what our data conists of. We want to see which variables are continuous vs which are categorical. After exploring some of the data, we see that we can create a feature. Number of sales divided by customers could give us a good metric to measure average sales per customer. We can also make an assumption that if we have missing values in this column that we have 0 customers. Since customers drive sales, we elect to remove all of these values.\nNotice the order in which the data is listed. It is ordered from most recent date to oldest date. This may cause a problem when we look to develop our model.\n   In\u0026nbsp;[3]: train.head() \n  \nOut[3]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Store DayOfWeek Sales Customers Open Promo StateHoliday SchoolHoliday   Date             2015-07-31 1 5 5263 555 1 1 0 1   2015-07-31 2 5 6064 625 1 1 0 1   2015-07-31 3 5 8314 821 1 1 0 1   2015-07-31 4 5 13995 1498 1 1 0 1   2015-07-31 5 5 4822 559 1 1 0 1      \n \n In\u0026nbsp;[4]: train.head() \n  \nOut[4]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Store DayOfWeek Sales Customers Open Promo StateHoliday SchoolHoliday   Date             2015-07-31 1 5 5263 555 1 1 0 1   2015-07-31 2 5 6064 625 1 1 0 1   2015-07-31 3 5 8314 821 1 1 0 1   2015-07-31 4 5 13995 1498 1 1 0 1   2015-07-31 5 5 4822 559 1 1 0 1      \n \n In\u0026nbsp;[5]: train.shape \n  \nOut[5]: (1017209, 8)  \n \n After that we will use the amazing .describe() function which can provide us most of the statistic elements.\n   In\u0026nbsp;[6]: train.describe() \n  \nOut[6]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Store DayOfWeek Sales Customers Open Promo SchoolHoliday     count 1.017209e+06 1.017209e+06 1.017209e+06 1.017209e+06 1.017209e+06 1.017209e+06 1.017209e+06   mean 5.584297e+02 3.998341e+00 5.773819e+03 6.331459e+02 8.301067e-01 3.815145e-01 1.786467e-01   std 3.219087e+02 1.997391e+00 3.849926e+03 4.644117e+02 3.755392e-01 4.857586e-01 3.830564e-01   min 1.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00   25% 2.800000e+02 2.000000e+00 3.727000e+03 4.050000e+02 1.000000e+00 0.000000e+00 0.000000e+00   50% 5.580000e+02 4.000000e+00 5.744000e+03 6.090000e+02 1.000000e+00 0.000000e+00 0.000000e+00   75% 8.380000e+02 6.000000e+00 7.856000e+03 8.370000e+02 1.000000e+00 1.000000e+00 0.000000e+00   max 1.115000e+03 7.000000e+00 4.155100e+04 7.388000e+03 1.000000e+00 1.000000e+00 1.000000e+00      \n \n We will check all the missing elements over here.\n   In\u0026nbsp;[7]: missing = train.isnull().sum() missing.sort_values(ascending=False) \n  \nOut[7]: SchoolHoliday 0 StateHoliday 0 Promo 0 Open 0 Customers 0 Sales 0 DayOfWeek 0 Store 0 dtype: int64  \n \n Next, we create a new metric to see average sales per customer.\n   In\u0026nbsp;[8]: train[\u0026#39;SalesPerCustomer\u0026#39;] = train[\u0026#39;Sales\u0026#39;]/train[\u0026#39;Customers\u0026#39;] train[\u0026#39;SalesPerCustomer\u0026#39;].head() \n  \nOut[8]: Date 2015-07-31 9.482883 2015-07-31 9.702400 2015-07-31 10.126675 2015-07-31 9.342457 2015-07-31 8.626118 Name: SalesPerCustomer, dtype: float64  \n \n We are going to Check if there are any missing values with our new metric and drop them. Either the customers or the sales should be zero to give us a null SalesPerCustomer.\n   In\u0026nbsp;[9]: missing = train.isnull().sum() missing.sort_values(ascending=False) \n  \nOut[9]: SalesPerCustomer 172869 SchoolHoliday 0 StateHoliday 0 Promo 0 Open 0 Customers 0 Sales 0 DayOfWeek 0 Store 0 dtype: int64  \n \n In\u0026nbsp;[10]: train.dropna().head() \n  \nOut[10]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Store DayOfWeek Sales Customers Open Promo StateHoliday SchoolHoliday SalesPerCustomer   Date              2015-07-31 1 5 5263 555 1 1 0 1 9.482883   2015-07-31 2 5 6064 625 1 1 0 1 9.702400   2015-07-31 3 5 8314 821 1 1 0 1 10.126675   2015-07-31 4 5 13995 1498 1 1 0 1 9.342457   2015-07-31 5 5 4822 559 1 1 0 1 8.626118      \n \n \nExploratory Data Analysis (Store Data)\u0026#182; We do the same as we did for our training set. Exploring the data, we see that there are only 3 missing values in CompetitionDistance. Because this is such a small amount, we elect to replace these with the mean of the column. The other missing values are all dependent on Promo2. Because these missing values are because Promo2 is equal to 0, we can replace these nulls with 0.\n   In\u0026nbsp;[11]: store.head() \n  \nOut[11]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval     0 1 c a 1270.0 9.0 2008.0 0 NaN NaN NaN   1 2 a a 570.0 11.0 2007.0 1 13.0 2010.0 Jan,Apr,Jul,Oct   2 3 a a 14130.0 12.0 2006.0 1 14.0 2011.0 Jan,Apr,Jul,Oct   3 4 c c 620.0 9.0 2009.0 0 NaN NaN NaN   4 5 a a 29910.0 4.0 2015.0 0 NaN NaN NaN      \n \n In\u0026nbsp;[12]: store.shape \n  \nOut[12]: (1115, 10)  \n \n In\u0026nbsp;[13]: store.isnull().sum() \n  \nOut[13]: Store 0 StoreType 0 Assortment 0 CompetitionDistance 3 CompetitionOpenSinceMonth 354 CompetitionOpenSinceYear 354 Promo2 0 Promo2SinceWeek 544 Promo2SinceYear 544 PromoInterval 544 dtype: int64  \n \n Since there are only 3 missing values from this, we fill with the average from the column\n   In\u0026nbsp;[14]: store[\u0026#39;CompetitionDistance\u0026#39;].fillna(store[\u0026#39;CompetitionDistance\u0026#39;].mean(), inplace = True) \n  \n The rows that do not have any Promo2 we can fill the rest of the values with 0\n   In\u0026nbsp;[15]: store.fillna(0, inplace = True) \n  \n In\u0026nbsp;[16]: store.head() \n  \nOut[16]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    Store StoreType Assortment CompetitionDistance CompetitionOpenSinceMonth CompetitionOpenSinceYear Promo2 Promo2SinceWeek Promo2SinceYear PromoInterval     0 1 c a 1270.0 9.0 2008.0 0 0.0 0.0 0   1 2 a a 570.0 11.0 2007.0 1 13.0 2010.0 Jan,Apr,Jul,Oct   2 3 a a 14130.0 12.0 2006.0 1 14.0 2011.0 Jan,Apr,Jul,Oct   3 4 c c 620.0 9.0 2009.0 0 0.0 0.0 0   4 5 a a 29910.0 4.0 2015.0 0 0.0 0.0 0      \n \n Join the data together using an inner join so only the data that is in both data set is joined\n   In\u0026nbsp;[17]: train = train.merge(right=store, on=\u0026#39;Store\u0026#39;, how=\u0026#39;left\u0026#39;) \n  \n \nMoving-Average Model (Naive Model)\u0026#182; We are going to be using a moving average model for the stock prediction of GM for our baseline model. The moving average model will take the average of different \u0026ldquo;windows\u0026rdquo; of time to come up with its forecast\nWe reload the data because now we have a sense of how we want to maniplate it for our model. After doing the same data manipulation as before, we start to look at the trend of our sales.\n   In\u0026nbsp;[19]: train = pd.read_csv(\u0026quot;https://raw.githubusercontent.com/RPI-DATA/tutorials-intro/master/rossmann-store-sales/rossmann-store-sales/train.csv\u0026quot;, parse_dates = True, low_memory = False, index_col = \u0026#39;Date\u0026#39;) train = train.sort_index(ascending = True) \n  \n In\u0026nbsp;[20]: train[\u0026#39;SalesPerCustomer\u0026#39;] = train[\u0026#39;Sales\u0026#39;]/train[\u0026#39;Customers\u0026#39;] train[\u0026#39;SalesPerCustomer\u0026#39;].head() \n  \nOut[20]: Date 2013-01-01 NaN 2013-01-01 NaN 2013-01-01 NaN 2013-01-01 NaN 2013-01-01 NaN Name: SalesPerCustomer, dtype: float64  \n \n In\u0026nbsp;[21]: train = train.dropna() \n  \n Here, we are simply graphing the sales that we have. As you can see, there are a tremendous amount of sales to the point where our graph just looks like a blue shading. However, we can get a sense of how our sales are distributed.\n   In\u0026nbsp;[22]: plt.plot(train.index, train[\u0026#39;Sales\u0026#39;]) plt.title(\u0026#39;Rossmann Sales\u0026#39;) plt.ylabel(\u0026#39;Price ($)\u0026#39;); plt.show() \n  \n  \n \n To clean up our graph, we want to form a new column which only accounts for the year of the sales.\n   In\u0026nbsp;[23]: train[\u0026#39;Year\u0026#39;] = train.index.year\n# Take Dates from index and move to Date column  train.reset_index(level=0, inplace = True) train[\u0026#39;sales\u0026#39;] = 0 \n  \n Split the data into a train and test set. We use an 80\u0026frasl;20 split. Then, we look to start are modein.\ntest_store stands for the forecasting part.\n   In\u0026nbsp;[24]: train_store=train[0:675472] test_store=train[675472:] \n  \n In\u0026nbsp;[25]: train_store.Date = pd.to_datetime(train_store.Date, format=\u0026quot;%Y-%m-%d\u0026quot;) train_store.index = train_store.Date test_store.Date = pd.to_datetime(test_store.Date, format=\u0026quot;%Y-%m-%d\u0026quot;) test_store.index = test_store.Date\ntrain_store = train_store.resample(\u0026#39;D\u0026#39;).mean() train_store = train_store.interpolate(method=\u0026#39;linear\u0026#39;)\ntest_store = test_store.resample(\u0026#39;D\u0026#39;).mean() test_store = test_store.interpolate(method=\u0026#39;linear\u0026#39;) \n  \n /Users/josefigueroa/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy self[name] = value     \n In\u0026nbsp;[26]: train_store.Sales.plot(figsize=(25,10), title=\u0026#39;daily sales\u0026#39;, fontsize=25) test_store.Sales.plot() \n  \nOut[26]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1c3f5b00\u0026gt;  \n  \n \n In\u0026nbsp;[27]: y_hat_avg_moving = test_store.copy() y_hat_avg_moving[\u0026#39;moving_avg_forcast\u0026#39;] = train_store.Sales.rolling(90).mean().iloc[-1] plt.figure(figsize=(25,10)) plt.plot(train_store[\u0026#39;Sales\u0026#39;], label=\u0026#39;Train\u0026#39;) plt.plot(test_store[\u0026#39;Sales\u0026#39;], label=\u0026#39;Test\u0026#39;) plt.plot(y_hat_avg_moving[\u0026#39;moving_avg_forcast\u0026#39;], label=\u0026#39;Moving Forecast\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.title(\u0026#39;Moving Average Forecast\u0026#39;) \n  \nOut[27]: Text(0.5, 1.0, \u0026#39;Moving Average Forecast\u0026#39;)  \n  \n \n In\u0026nbsp;[28]: rms_avg_rolling = sqrt(mean_squared_error(test_store.Sales, y_hat_avg_moving.moving_avg_forcast)) print(\u0026#39;ROLLING AVERAGE\u0026#39;,rms_avg_rolling) \n  \n ROLLING AVERAGE 1915.886219620586     \n The rolling average for our model is 1,915.88. This prediction seems to be very consistent in hitting the average of the future sales. This naive model definitely looks like a solid model, however, it is not the best one.\n   \nFacebook Prophet Model\u0026#182; The Facebook Prophet package is designed to analyze time series data with daily observations, which can display patterns on different time scales. Prophet is optimized for business tasks with the following characteristics:\nhourly, daily, or weekly observations with at least a few months (preferably a year) of history strong multiple “human-scale” seasonalities: day of week and time of year important holidays that occur at irregular intervals that are known in advance (e.g. the Super Bowl) a reasonable number of missing observations or large outliers historical trend changes, for instance due to product launches or logging changes trends that are non-linear growth curves, where a trend hits a natural limit or saturates inspired by https://research.fb.com/prophet-forecasting-at-scale/\n   According to the \u0026ldquo;facebook research\u0026rdquo; website, there is four main component inside the facebook prophet model.\n** A piecewise linear or logistic growth trend.\n** A yearly seasonal component modeled using Fourier series.\n** A weekly seasonal component using dummy variables.\n** A user-provided list of important holidays.\nThe method of combing different models into one makes the facebook prophet model much more precise and flexible.\n   First of all, we will dealt with the data as same as the naive model\n   In\u0026nbsp;[31]: train = pd.read_csv(\u0026quot;https://raw.githubusercontent.com/RPI-DATA/tutorials-intro/master/rossmann-store-sales/rossmann-store-sales/train.csv\u0026quot;, parse_dates = True, low_memory = False) \n  \n In\u0026nbsp;[32]: train[\u0026#39;SalesPerCustomer\u0026#39;] = train[\u0026#39;Sales\u0026#39;]/train[\u0026#39;Customers\u0026#39;] train[\u0026#39;SalesPerCustomer\u0026#39;].head()\ntrain = train.dropna() \n  \n In\u0026nbsp;[33]: sales = train[train.Store == 1].loc[:, [\u0026#39;Date\u0026#39;, \u0026#39;Sales\u0026#39;]]\n# reverse to the order: from 2013 to 2015 sales = sales.sort_index(ascending = False)\nsales[\u0026#39;Date\u0026#39;] = pd.DatetimeIndex(sales[\u0026#39;Date\u0026#39;]) sales.dtypes \n  \nOut[33]: Date datetime64[ns] Sales int64 dtype: object  \n \n In\u0026nbsp;[34]: sales = sales.rename(columns = {\u0026#39;Date\u0026#39;: \u0026#39;ds\u0026#39;, \u0026#39;Sales\u0026#39;: \u0026#39;y\u0026#39;}) \n  \n In\u0026nbsp;[35]: sales.head() \n  \nOut[35]:  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    ds y     1014980 2013-01-02 5530   1013865 2013-01-03 4327   1012750 2013-01-04 4486   1011635 2013-01-05 4997   1009405 2013-01-07 7176      \n \n Now, despite of the naive model, we will apply our sales set to the face book model by using the function fbprophet. fbprophet.Prophet can change the value of \u0026ldquo;changepoint_prior_scale\u0026rdquo; to 0.05 to achieve a better fit or to 0.15 to control how sensitive the trend is.\n   In\u0026nbsp;[36]: sales_prophet = fbprophet.Prophet(changepoint_prior_scale=0.05, daily_seasonality=True) sales_prophet.fit(sales) \n  \nOut[36]: \u0026lt;fbprophet.forecaster.Prophet at 0x1a1c952da0\u0026gt;  \n \n We will figure out the best forecasting by changing the value of changepoints.\nIf we find that our model is is fitting too closely to our training data (overfitting), our data will not be able to generalize new data.\nIf our model is not fitting closely enough to our training data (underfitting), our data has too much bias.\nUnderfitting: increase changepoint to allow more flexibility Overfitting: decrease changepoint to limit flexibili\n   In\u0026nbsp;[37]: # Try 4 different changepoints for changepoint in [0.001, 0.05, 0.1, 0.5]: model = fbprophet.Prophet(daily_seasonality=False, changepoint_prior_scale=changepoint) model.fit(sales)\n\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;future\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;model\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;make_future_dataframe\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;periods\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;mi\u0026quot;\u0026gt;365\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;,\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;freq\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;s1\u0026quot;\u0026gt;\u0026amp;#39;D\u0026amp;#39;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;future\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;model\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;.\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;predict\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;(\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;future\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;)\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;sales\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;changepoint\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;]\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;o\u0026quot;\u0026gt;=\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;n\u0026quot;\u0026gt;future\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;[\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;s1\u0026quot;\u0026gt;\u0026amp;#39;yhat\u0026amp;#39;\u0026lt;/span\u0026gt;\u0026lt;span class=\u0026quot;p\u0026quot;\u0026gt;]\u0026lt;/span\u0026gt;  \n  \n We can now create the plot under all of the situation.\n   In\u0026nbsp;[38]: # Create the plot plt.figure(figsize=(10, 8))\n# Actual observations plt.plot(sales[\u0026#39;ds\u0026#39;], sales[\u0026#39;y\u0026#39;], \u0026#39;ko\u0026#39;, label = \u0026#39;Observations\u0026#39;) colors = {0.001: \u0026#39;b\u0026#39;, 0.05: \u0026#39;r\u0026#39;, 0.1: \u0026#39;grey\u0026#39;, 0.5: \u0026#39;gold\u0026#39;}\n# Plot each of the changepoint predictions for changepoint in [0.001, 0.05, 0.1, 0.5]: plt.plot(sales[\u0026#39;ds\u0026#39;], sales[changepoint], color = colors[changepoint], label = \u0026#39;%.3fprior scale\u0026#39; % changepoint)\nplt.legend(prop={\u0026#39;size\u0026#39;: 14}) plt.xlabel(\u0026#39;Date\u0026#39;); plt.ylabel(\u0026#39;Rossmann Sales\u0026#39;); plt.title(\u0026#39;Rossmann Effect of Changepoint Prior Scale\u0026#39;); \n  \n  \n \n Predictions for 6 Weeks In order to make forecasts, we need to create a future dataframe. We need to specify the amount of future periods to predict and the frequency of our prediction.\nPeriods: 6 Weeks\nFrequency: Daily\n   In\u0026nbsp;[39]: # Make a future dataframe for 6weeks sales_forecast = sales_prophet.make_future_dataframe(periods=6*7, freq=\u0026#39;D\u0026#39;) # Make predictions sales_forecast = sales_prophet.predict(sales_forecast) \n  \n In\u0026nbsp;[40]: sales_prophet.plot(sales_forecast, xlabel = \u0026#39;Date\u0026#39;, ylabel = \u0026#39;Sales\u0026#39;) plt.title(\u0026#39;Rossmann Sales\u0026#39;); \n  \n  \n \n In\u0026nbsp;[41]: sales_prophet.changepoints[:10] \n  \nOut[41]: 25 2013-01-31 50 2013-03-01 75 2013-04-02 100 2013-05-02 125 2013-06-04 150 2013-07-03 174 2013-07-31 199 2013-08-29 224 2013-09-27 249 2013-10-28 Name: ds, dtype: datetime64[ns]  \n \n We have listed out the most significant changepoints in our data. This is representing when the time series growth rate significantly changes.\n   \nConclusion\u0026#182; In this notebook, we made 2 different math model for the rossmann store sales dataset to forecast the future sales. Moving-average model brings us a basic understand of how the math model works, while facebook prophet model calculates the best solid result. Those math model will give us both of the rolling average and test model.\n   \nReferences\u0026#182; The dataset is the rossmann store sales dataset from kaggle: https://www.kaggle.com/c/rossmann-store-sales\nFacebook Prophet Documentation: https://research.fb.com/prophet-forecasting-at-scale/\n   Contributers\u0026#182; nickespo21 Linghao Dong Jose Figueroa \n   \n  \n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/podcasts/",
	"title": "Podcasts",
	"tags": [],
	"description": "",
	"content": " Linear Digressions  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/tech/",
	"title": "Technology",
	"tags": [],
	"description": "",
	"content": " Often technical issues around computer configuration can be frustrating, but they are an important component of doing data science. You will need a number of different systems setup on your computer for this class.\nWe will be testing this semester a online JupyterHub instance that should simplify a lot of the setup. However this is still in an alpha release and it would be good to get an alternate compute environment setup on your laptop.\nIn describing options, there are cloud and laptop based ways of doing the exercises.\nCloud A cloud based way of doing data science doesn\u0026rsquo;t require you to install anything and you can be up and running in working with concepts asap.\nOur Cloud: lab.analyticsdojo.com For the cloud instance at lab.analyticsdojo.com, there is some cool technology that is driving this. We are utilizing Kubernetes on the Google Cloud Platform to run Jupyterhub. This is based on some amazing work by a talented community of open source developers.\nWhen someone logs into the platform, the students obtain their own container to run on, giving them access to a compute environment that is much more flexible than a typical brower.\n##Laptop When running on your laptop, there are 2 different configuration options that that are possible.\nFor most users, managing the analytics environment with Anaconda seems to work quite well. Anaconda is a package manager with detailed instructions for Mac/Windows/Linux users. In this clase\nIt is also possible to run Docker on your laptop, mirroring the same compute environment that you enjoyed on JupyterHub cloud environment. This has worked perfectly in the past for Mac users and not so well for Windows users, but I understand Docker for Windows has come a long way.\nGit and Github Git is a version control system use to track changes to code, and when combined with Github it is an extremely powerful way to collaborate.\n(1) Install the git command line tool. There are a lot of options but you should be able to just utilize the defaults.\n(2) Create an account on github.com.\n(3) Install Github desktop if using OSX or Windows. Click here for download instructions for Github Desktop.\n(4) Once you are done installing Github desktop, open the program. Sign in with your Github Credentials to link the desktop application with your Github account.\n(5) Clone the class repository either from the desktop tool or from the command line.\nFrom the Github tool: From the command line:\ngit clone https://github.com/jkuruzovich/techfundamentals-spring2018-materials  (6) In the future when you want to update the repository, you can do so either from the command line or the GitHub Desktop tool.\nFrom the GitHub Desktop, press the sync button after selecting the repository: From the command line:\ngit pull  Step 2: Install and Configure Anaconda We will be using Anaconda to provide a Jupyter environment for both R and Python.\nFollow the following instructions to install Anaconda from Continuum Analytics. OSX Windows Linux\nOnce you have the Anaconda Distribution, follow below for advanced configuration.\nOSX  Follow the online instructions to install the Anaconda Distribution with Python 3.6 Follow below for advanced configuration.  Windows Windows may often have issues associated with installation if you have had previous versions of Anaconda installed. If it doesn\u0026rsquo;t work, start by uninstalling everything\n Follow the online instructions to install the Anaconda Distribution with Python 3.6 Follow below for advanced configuration.  Linux If you are a Linux user, please help in supporting the class. You likely won\u0026rsquo;t need my help. ;) 1. Follow the online instructions to install the Anaconda Distribution with Python 3.6 2. Follow below for advanced configuration.\nAdvanced Configuration (a.) Windows users, open an \u0026lsquo;Anaconda Prompt\u0026rsquo;. Mac/Linux users open a standard terminal.\n(b.) Change directories to the techfundamentals-spring2018-materials folder you cloned earlier.\ncd \u0026lt;insert full path \u0026gt;\\techfundamentals-spring2018-materials  For the above, changing directories differs for Windows or Mac users. For example, on a Mac it might be:\ncd /Users/jasonkuruzovich/githubdesktop/techfundamentals-spring2018-materials  On Windows, if you have a space somewhere in your path make sure that you include quotes:\ncd \u0026quot;c:\\Users\\jasonkuruzovich\\github\\techfundamentals-spring2018-materials\u0026quot;  (c.) Install required packages. First install pip, which is a package manager similar to conda.\nconda install -c anaconda pip  Then, from the repository install all the packages in requirements.txt.\npip install -r requirements.txt  (d.) Launch a Jupyter notebook with the command:\njupyter notebook  You will need to run c-d each time to launch Jupyter.\n*There may be some other packages you still have to install. You will know this is an issue if an import command fails. You can install packages with:\nconda install \u0026lt;packagename\u0026gt;  or\npip install \u0026lt;packagename\u0026gt;  For example, to install the R Kernal for Jupyter, install the r-essentials package:\nconda install r-essentials  Please feel free to post issues on the Slack channel.\nFollow the instructions to install git/GitHub Desktop and Anaconda.\n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": " Pro Git\n Codeschool Git Class\n Learning about Branches\n Explaining Git with D3\n Understanding the Git Flow\n  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": "  Introduction to Machine Learning with Python by Andreas C. Müller; Sarah Guido (Available through library) Think Stats Probability and Statistics for Programmers in Python (free download)\n Conda\n Python + Numpy Tutorial\n Python Docs Tutorial\nPython: Conda\n Python + Numpy Tutorial\n Python Docs Tutorial\n Plotting:\n matplotlib.pyplot tutorial: This short tutorial provides an overview of the basic plotting utilities we will be using.\n seaborn: The Seaborn library has some nice additional visualization functions that we may use occasionally.\n Pandas:\n Learn Pandas A set of lessons providing an overview of the Pandas library.\n Python for Data Science Another set of notebook demonstrating Pandas functionality.\n  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/r/",
	"title": "R",
	"tags": [],
	"description": "",
	"content": " Introduction to Statistical Learning (Free online PDF)\n R for Data Science (available online)\n Berkeley R Bootcamp\n  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/resources/books/",
	"title": "Books",
	"tags": [],
	"description": "",
	"content": " O\u0026rsquo;Reilly Media titles can be accessed through the RPI library subscription. If there is a specific title you would like access to that isn\u0026rsquo;t listed, the library can adjust the list.\nData Science \u0026amp; Business Data Science for Business \nDoing Data Science\nGit  Pro Git  PYTHON  Introduction to Machine Learning with Python Data Science Handbook Data Science from Scratch  R  Introduction to Statistical Learning (Free online PDF) R for Data Science (available online)\n Introduction to Machine Learning with Python by Andreas C. Müller; Sarah Guido (Available through library)\n Think Stats Probability and Statistics for Programmers in Python (free download)\n  Deep Learning  Hands-On Machine Learning with Scikit-Learn and TensorFlow by Aurélien Géron  Big Data  Mining Massive Datasets  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": " Frequently Asked Question:  What is the Data@Rensselaer ?  Data@Rensselaer is an open resource for all rpi members to use. We are trying to use data since to create a new way for people to coordinate learning.\n Is their any requirement for people to use Data@Rensselaer ?  Every one can use our notebook to get a general learning experience. Most of our notebooks are written in python. People with coding experience can take full advantage of our notebooks and the code behind. Also, we do have the prerequsite for some of the notebook.\n What kind of topic will be presented ?  We will go through the topics in engineering and science such as linear regression.\n Do we have to download the software(Anaconda/ jupyter notebook) to use the notebook ?  You don\u0026rsquo;t need to download any software to use our resource. You can go through all the notebook by our website or collab. For people who want to modify our notebook or use the extra assignment notebook for learning, downloading the jupyter notebook is required.\n How can i use those notebooks ?  People can use those notebook for learning and teaching. We do allow people modify our notebooks under certain license.\n Can i be a contributor for Data@Rensselaer ? \u0026hellip;  "
},
{
	"uri": "http://rpi-data.analyticsdojo.com/",
	"title": "DATA@RENSSELAER",
	"tags": [],
	"description": "Technology and Infrastructure for Teaching Data &amp; Computation Courses",
	"content": "Welcome to DATA@RENSSELAR, a platform for enabling data science across the curriculum.\nWe are an open source project that helps analytics programs to both create a community and deliver the highest quality education experience in the rapidly changing world of analytics and data science.\nCurrently the learning stack associated with these courses includes:\n Jupyter GitHub Classroom Slack [coming soon] autograding  Our idea is by adopting a common technology stack across different classes, we will be better able to coordinate learning.\nContact Us:\nEmail lewisd2@rpi.edu and kuruzj@rpi.edu\n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "DATA@RENSSELAER was funded by the teaching and learning collaboratory.\n"
},
{
	"uri": "http://rpi-data.analyticsdojo.com/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://rpi-data.analyticsdojo.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://rpi-data.analyticsdojo.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]